{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nestlé HR Assistant Setup\n",
    "\n",
    "1. Load Nestlé HR PDF  \n",
    "2. Split into chunks with PyPDFLoader  \n",
    "3. Create embeddings & vectorstore  \n",
    "4. Build QA retrieval chain  \n",
    "5. Launch Gradio interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/sheilamcgovern/Desktop/Projects2025/nestle_hr_assistant\n",
      "data/raw contains: ['the_nestle_hr_policy_pdf_2012.pdf', '.ipynb_checkpoints']\n",
      "Loading: /Users/sheilamcgovern/Desktop/Projects2025/nestle_hr_assistant/data/raw/the_nestle_hr_policy_pdf_2012.pdf\n",
      "Loaded 8 pages\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ensure we’re at the project root\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "\n",
    "# Show what’s in data/raw\n",
    "raw_dir = os.path.join(os.getcwd(), \"data\", \"raw\")\n",
    "print(\"data/raw contains:\", os.listdir(raw_dir))\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Adjust this to match the exact filename you see above\n",
    "pdf_filename = \"the_nestle_hr_policy_pdf_2012.pdf\"\n",
    "pdf_path = os.path.join(raw_dir, pdf_filename)\n",
    "print(\"Loading:\", pdf_path)\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} pages\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 20 chunks\n",
      "CSV written.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# assume `docs` is already loaded\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks   = splitter.split_documents(docs)\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\"chunk_id\": i, \"page\": c.metadata.get(\"page\"), \"text\": c.page_content.replace(\"\\n\",\" \")}\n",
    "    for i, c in enumerate(chunks)\n",
    "])\n",
    "df.head(10)\n",
    "\n",
    "#—and if you want to save:\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "df.to_csv(\"data/processed/hr_policy_chunks.csv\", index=False)\n",
    "print(\"CSV written.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CHROMA_DISABLE_TELEMETRY\"] = \"1\" # for chroma telemetry warnings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "\n",
    "# rebuild docs from CSV\n",
    "df = pd.read_csv(\"data/processed/hr_policy_chunks.csv\")\n",
    "docs = [\n",
    "    Document(page_content=row[\"text\"], metadata={\"page\": row[\"page\"], \"chunk_id\": int(row[\"chunk_id\"])})\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# (re)create vector store & QA chain\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb   = Chroma.from_documents(docs, embeddings, persist_directory=\"db/chroma\")\n",
    "qa_chain   = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "\n",
    "def respond(message, history):\n",
    "    answer = qa_chain.run(message)\n",
    "    history = history + [(message, answer)]\n",
    "    return history, \"\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Nestlé HR Assistant\\nAsk anything about Nestlé’s HR policy documents.\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    txt     = gr.Textbox(show_label=False, placeholder=\"Type your question and hit enter\")\n",
    "    txt.submit(respond, [txt, chatbot], [chatbot, txt])\n",
    "    # optional clear button:\n",
    "    # clear = gr.Button(\"Clear\")\n",
    "    # clear.click(lambda: [], None, chatbot)\n",
    "\n",
    "demo.launch()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_w/1v5jxkz56qxcjphx0msmcxr00000gn/T/ipykernel_32620/50595359.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(qa_chain.run(\"What are the working hours?\"))\n",
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have specific information about the working hours from the context provided. It would be best to refer to the company's official documentation or speak directly with the HR department for details on working hours.\n"
     ]
    }
   ],
   "source": [
    "print(qa_chain.run(\"What are the working hours?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_w/1v5jxkz56qxcjphx0msmcxr00000gn/T/ipykernel_32620/2499365149.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(\"working hours\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 candidate chunks:\n",
      "• page 4, chunk 10: working inside or outside our premises under  contractual obligations with service providers  and we insist that they also take steps so that  adequate working conditions are made available  to them. \n",
      "---\n",
      "• page 4, chunk 10: working inside or outside our premises under  contractual obligations with service providers  and we insist that they also take steps so that  adequate working conditions are made available  to them. \n",
      "---\n",
      "• page 4, chunk 10: working inside or outside our premises under  contractual obligations with service providers  and we insist that they also take steps so that  adequate working conditions are made available  to them. \n",
      "---\n",
      "• page 4, chunk 10: working inside or outside our premises under  contractual obligations with service providers  and we insist that they also take steps so that  adequate working conditions are made available  to them. \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# debug what your retriever is pulling back\n",
    "retriever = vectordb.as_retriever()\n",
    "docs = retriever.get_relevant_documents(\"working hours\")\n",
    "\n",
    "print(f\"Found {len(docs)} candidate chunks:\")\n",
    "for d in docs:\n",
    "    page = d.metadata.get(\"page\")\n",
    "    chunk_id = d.metadata.get(\"chunk_id\")\n",
    "    snippet = d.page_content.replace(\"\\n\", \" \")[:200]\n",
    "    print(f\"• page {page}, chunk {chunk_id}: {snippet}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 candidate chunks:\n",
      "• page 4, chunk 10\n",
      "   working inside or outside our premises under  contractual obligations with service providers  and we insist that they also take steps so that  adequate working conditions are made available  to them. \n",
      "---\n",
      "• page 4, chunk 10\n",
      "   working inside or outside our premises under  contractual obligations with service providers  and we insist that they also take steps so that  adequate working conditions are made available  to them. \n",
      "---\n",
      "• page 4, chunk 10\n",
      "   working inside or outside our premises under  contractual obligations with service providers  and we insist that they also take steps so that  adequate working conditions are made available  to them. \n",
      "---\n",
      "• page 4, chunk 10\n",
      "   working inside or outside our premises under  contractual obligations with service providers  and we insist that they also take steps so that  adequate working conditions are made available  to them. \n",
      "---\n",
      "• page 4, chunk 10\n",
      "   working inside or outside our premises under  contractual obligations with service providers  and we insist that they also take steps so that  adequate working conditions are made available  to them. \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# rebuild the retriever to pull back 5 documents\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# now ask Chroma for relevant chunks\n",
    "docs = retriever.get_relevant_documents(\"hours of work\")\n",
    "\n",
    "print(f\"Found {len(docs)} candidate chunks:\")\n",
    "for d in docs:\n",
    "    print(f\"• page {d.metadata.get('page')}, chunk {d.metadata.get('chunk_id')}\")\n",
    "    print(\"  \", d.page_content.replace('\\n',' ')[:200])\n",
    "    print(\"---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique hits for 'hours of work':\n",
      "• page 4, chunk 10\n",
      "   working inside or outside our premises under  contractual obligations with service providers  and we insist that they also take steps so that  adequate working conditions are made available  to them. \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# embed & search directly, then dedupe by chunk_id\n",
    "query = \"hours of work\"\n",
    "results = vectordb.similarity_search(query, k=5)\n",
    "\n",
    "unique = {}\n",
    "for d in results:\n",
    "    cid = d.metadata[\"chunk_id\"]\n",
    "    if cid not in unique:\n",
    "        unique[cid] = d\n",
    "\n",
    "print(f\"Unique hits for '{query}':\")\n",
    "for d in unique.values():\n",
    "    print(f\"• page {d.metadata['page']}, chunk {d.metadata['chunk_id']}\")\n",
    "    print(\"  \", d.page_content.replace('\\n',' ')[:200])\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have specific information on what the Nestlé Human Resources Policy says about workplace safety.\n"
     ]
    }
   ],
   "source": [
    "print(qa_chain.run(\"What does the policy say about workplace safety?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
