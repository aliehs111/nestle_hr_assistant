{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nestlé HR Assistant Setup\n",
    "\n",
    "1. Load Nestlé HR PDF  \n",
    "2. Split into chunks with PyPDFLoader  \n",
    "3. Create embeddings & vectorstore  \n",
    "4. Build QA retrieval chain  \n",
    "5. Launch Gradio interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/sheilamcgovern/Desktop/Projects2025/nestle_hr_assistant\n",
      "data/raw contains: ['the_nestle_hr_policy_pdf_2012.pdf', '.ipynb_checkpoints']\n",
      "Loading: /Users/sheilamcgovern/Desktop/Projects2025/nestle_hr_assistant/data/raw/the_nestle_hr_policy_pdf_2012.pdf\n",
      "Loaded 8 pages\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ensure we’re at the project root\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "\n",
    "# Show what’s in data/raw\n",
    "raw_dir = os.path.join(os.getcwd(), \"data\", \"raw\")\n",
    "print(\"data/raw contains:\", os.listdir(raw_dir))\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Adjust this to match the exact filename you see above\n",
    "pdf_filename = \"the_nestle_hr_policy_pdf_2012.pdf\"\n",
    "pdf_path = os.path.join(raw_dir, pdf_filename)\n",
    "print(\"Loading:\", pdf_path)\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} pages\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 60 chunks\n",
      "Unique chunks: 60\n",
      "CSV written.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assume `docs` is loaded from PDF (execution_count 1)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "\n",
    "# Deduplicate and fix page numbers\n",
    "chunk_data = []\n",
    "seen_content = set()\n",
    "for i, c in enumerate(chunks):\n",
    "    content = c.page_content.replace(\"\\n\", \" \").strip()\n",
    "    if content not in seen_content:\n",
    "        seen_content.add(content)\n",
    "        chunk_data.append({\n",
    "            \"chunk_id\": i,\n",
    "            \"page\": c.metadata.get(\"page\") + 1,\n",
    "            \"text\": content\n",
    "        })\n",
    "df = pd.DataFrame(chunk_data)\n",
    "print(f\"Unique chunks: {len(df)}\")\n",
    "\n",
    "# Save to CSV\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "df.to_csv(\"data/processed/hr_policy_chunks.csv\", index=False)\n",
    "print(\"CSV written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded: Yes\n",
      "API Key preview: sk-pr...\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"API Key loaded:\", \"Yes\" if api_key else \"No\")\n",
    "if api_key:\n",
    "    print(\"API Key preview:\", api_key[:5] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event CollectionGetEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60 docs\n",
      "Stored 60 unique docs\n",
      "Docs in vector store: 60\n",
      "Sample metadata: {'chunk_id': 0, 'page': 1}\n",
      "Sample content: Policy Mandatory September  2012 The Nestlé   Human Resources Policy ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CHROMA_DISABLE_TELEMETRY\"] = \"1\"\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Load chunks\n",
    "df = pd.read_csv(\"data/processed/hr_policy_chunks.csv\")\n",
    "docs = [Document(page_content=row[\"text\"], metadata={\"page\": int(row[\"page\"]), \"chunk_id\": int(row[\"chunk_id\"])}) for _, row in df.iterrows()]\n",
    "print(f\"Loaded {len(docs)} docs\")\n",
    "\n",
    "# Create vector store\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vectordb = Chroma(collection_name=\"hr_policy_2012\", embedding_function=embeddings, persist_directory=\"db/chroma\")\n",
    "# Clear any existing data\n",
    "if vectordb.get()['ids']:\n",
    "    vectordb.delete_collection()\n",
    "    vectordb = Chroma(collection_name=\"hr_policy_2012\", embedding_function=embeddings, persist_directory=\"db/chroma\")\n",
    "# Add unique documents with unique IDs\n",
    "unique_docs = list({doc.page_content: doc for doc in docs}.values())\n",
    "ids = [f\"doc_{i}\" for i in range(len(unique_docs))]  # Unique IDs\n",
    "vectordb.add_documents(documents=unique_docs, ids=ids)\n",
    "print(f\"Stored {len(unique_docs)} unique docs\")\n",
    "\n",
    "# Verify content\n",
    "docs = vectordb.get()\n",
    "print(f\"Docs in vector store: {len(docs['ids'])}\")\n",
    "if docs['ids']:\n",
    "    print(\"Sample metadata:\", docs[\"metadatas\"][0])\n",
    "    print(\"Sample content:\", docs[\"documents\"][0][:100], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CHROMA_DISABLE_TELEMETRY\"] = \"1\"\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    # Load chunks\n",
    "    df = pd.read_csv(\"data/processed/hr_policy_chunks.csv\")\n",
    "    docs = [Document(page_content=row[\"text\"], metadata={\"page\": int(row[\"page\"]), \"chunk_id\": int(row[\"chunk_id\"])}) for _, row in df.iterrows()]\n",
    "    print(f\"Loaded {len(docs)} docs\")\n",
    "\n",
    "    # Create vector store\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vectordb = Chroma(collection_name=\"hr_policy_2012\", embedding_function=embeddings, persist_directory=\"db/chroma\")\n",
    "    unique_docs = list({doc.page_content: doc for doc in docs}.values())\n",
    "    ids = [f\"doc_{i}\" for i in range(len(unique_docs))]\n",
    "    vectordb.add_documents(documents=unique_docs, ids=ids)\n",
    "    print(f\"Stored {len(unique_docs)} docs\")\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"Answer using only the provided snippets. Quote exact text relevant to the question. If the snippets do not directly address the question, say: 'I don’t know based on the 2012 HR Policy document.'\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    # QA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectordb.as_retriever(search_kwargs={\"k\": 10}),\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    # Gradio\n",
    "    def respond(message, history=[]):\n",
    "        try:\n",
    "            print(f\"Processing query: {message}\")\n",
    "            if not message:\n",
    "                return history, \"Please enter a question.\"\n",
    "            result = qa_chain.invoke({\"query\": message})\n",
    "            answer = str(result[\"result\"])\n",
    "            source_docs = result[\"source_documents\"]\n",
    "            seen = set()\n",
    "            unique_sources = [doc for doc in source_docs if not (doc.metadata[\"chunk_id\"] in seen or seen.add(doc.metadata[\"chunk_id\"]))]\n",
    "            if unique_sources and \"I don’t know\" not in answer.lower():\n",
    "                pages = sorted({doc.metadata[\"page\"] for doc in unique_sources})\n",
    "                answer += f\"\\n\\n*Source: Page{'s' if len(pages) > 1 else ''} {', '.join(map(str, pages))}*\"\n",
    "            elif \"I don’t know\" in answer.lower():\n",
    "                answer += \"\\n\\n*Note: The 2012 policy lacks specific details on this topic. Contact Nestlé HR for current policies.*\"\n",
    "            history.append((str(message), answer))\n",
    "            print(f\"Returning history: {len(history)} messages\")\n",
    "            return history, \"\"\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            traceback.print_exc()\n",
    "            return history + [(str(message), error_msg)], \"\"\n",
    "\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"## Nestlé HR Assistant\\nAsk about 2012 HR Policy. Ex: 'What are the total rewards?'\")\n",
    "        chatbot = gr.Chatbot()\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Type question and hit enter\")\n",
    "        txt.submit(respond, [txt, chatbot], [chatbot, txt])\n",
    "\n",
    "    demo.launch()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradio version: 5.36.2\n"
     ]
    }
   ],
   "source": [
    "import gradio\n",
    "print(\"Gradio version:\", gradio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: total rewards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 5, Chunk 19: value and trust that our name brings to those  who work with us; the relationships with our line  ma...\n",
      "Page 5, Chunk 20: receive. Nestlé, therefore, focuses on Fixed Pay,  Variable Pay, Benefits, Personal Growth and  Deve...\n",
      "Page 5, Chunk 22: Nestlé Total Rewards programmes must be  established within the social and legal framework  of each ...\n",
      "Page 5, Chunk 24: transparency. Corporate policy:  Nestlé Total Rewards Policy We are committed to providing our emplo...\n",
      "Page 5, Chunk 18: The Nestlé Human Resources Policy 3  Total rewards Attracting new hires and keeping current  employe...\n",
      "\n",
      "Query: working hours\n",
      "Page 5, Chunk 30: employees is heard. Corporate policy:  Policy on Conditions of Work and Employment  Employment and w...\n",
      "Page 5, Chunk 28: and we insist that they also take steps so that  adequate working conditions are made available  to ...\n",
      "Page 7, Chunk 50: communication is established in the workplace.  While dialogue with trade unions is essential, it  d...\n",
      "Page 5, Chunk 23: within the framework of Company policy.  Sufficient time should be spent with each  employee to expl...\n",
      "Page 1, Chunk 0: Policy Mandatory September  2012 The Nestlé   Human Resources Policy...\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vectordb = Chroma(collection_name=\"hr_policy_2012\", embedding_function=embeddings, persist_directory=\"db/chroma\")\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "for query in [\"total rewards\", \"working hours\"]:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    docs = retriever.invoke(query)\n",
    "    for d in docs:\n",
    "        print(f\"Page {d.metadata['page']}, Chunk {d.metadata['chunk_id']}: {d.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: total rewards\n",
      "Page 5, Chunk 19: value and trust that our name brings to those  who work with us; the relationships with our line  ma...\n",
      "Page 5, Chunk 20: receive. Nestlé, therefore, focuses on Fixed Pay,  Variable Pay, Benefits, Personal Growth and  Deve...\n",
      "Page 5, Chunk 22: Nestlé Total Rewards programmes must be  established within the social and legal framework  of each ...\n",
      "Page 5, Chunk 24: transparency. Corporate policy:  Nestlé Total Rewards Policy We are committed to providing our emplo...\n",
      "Page 5, Chunk 18: The Nestlé Human Resources Policy 3  Total rewards Attracting new hires and keeping current  employe...\n",
      "\n",
      "Query: working hours\n",
      "Page 5, Chunk 30: employees is heard. Corporate policy:  Policy on Conditions of Work and Employment  Employment and w...\n",
      "Page 5, Chunk 28: and we insist that they also take steps so that  adequate working conditions are made available  to ...\n",
      "Page 7, Chunk 50: communication is established in the workplace.  While dialogue with trade unions is essential, it  d...\n",
      "Page 5, Chunk 23: within the framework of Company policy.  Sufficient time should be spent with each  employee to expl...\n",
      "Page 1, Chunk 0: Policy Mandatory September  2012 The Nestlé   Human Resources Policy...\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vectordb = Chroma(collection_name=\"hr_policy_2012\", embedding_function=embeddings, persist_directory=\"db/chroma\")\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "for query in [\"total rewards\", \"working hours\"]:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    docs = retriever.invoke(query)\n",
    "    for d in docs:\n",
    "        print(f\"Page {d.metadata['page']}, Chunk {d.metadata['chunk_id']}: {d.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response: Hello!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    response = llm.invoke(\"Test query: Say 'Hello'\")\n",
    "    print(\"API Response:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"OpenAI API error: {str(e)}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
