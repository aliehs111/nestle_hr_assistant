{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nestlé HR Assistant Setup\n",
    "\n",
    "1. Load Nestlé HR PDF  \n",
    "2. Split into chunks with PyPDFLoader  \n",
    "3. Create embeddings & vectorstore  \n",
    "4. Build QA retrieval chain  \n",
    "5. Launch Gradio interface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Situation\n",
    "\n",
    "Nestlé, as a leading multinational corporation, manages thousands of employees globally and relies on clear HR policies to ensure consistency and efficiency. However, HR staff often spend valuable time searching through long policy documents to answer employee questions.  \n",
    "To address this, I have been tasked with creating an AI-powered HR assistant that can quickly retrieve accurate information from Nestlé’s HR Policy (2012 edition). This assistant should streamline HR workflows and provide employees with fast, reliable answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/sheilamcgovern/Desktop/Projects2025/nestle_hr_assistant\n",
      "data/raw contains: ['the_nestle_hr_policy_pdf_2012.pdf', '.ipynb_checkpoints']\n",
      "Loading: /Users/sheilamcgovern/Desktop/Projects2025/nestle_hr_assistant/data/raw/the_nestle_hr_policy_pdf_2012.pdf\n",
      "Loaded 8 pages\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ensure we’re at the project root\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "\n",
    "# Show what’s in data/raw\n",
    "raw_dir = os.path.join(os.getcwd(), \"data\", \"raw\")\n",
    "print(\"data/raw contains:\", os.listdir(raw_dir))\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Adjust this to match the exact filename you see above\n",
    "pdf_filename = \"the_nestle_hr_policy_pdf_2012.pdf\"\n",
    "pdf_path = os.path.join(raw_dir, pdf_filename)\n",
    "print(\"Loading:\", pdf_path)\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} pages\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "My task is to develop a conversational chatbot that can answer employee questions about Nestlé’s HR Policy (2012).  \n",
    "The chatbot should:\n",
    "- Extract and process the PDF policy document.\n",
    "- Represent the content in a format that supports fast, accurate retrieval.\n",
    "- Use OpenAI’s GPT model to provide well-structured answers.\n",
    "- Provide a user-friendly interface through Gradio.\n",
    "- Clearly cite sources (page numbers) when giving responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 60 chunks\n",
      "Unique chunks: 60\n",
      "CSV written.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assume `docs` is loaded from PDF (execution_count 1)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "\n",
    "# Deduplicate and fix page numbers\n",
    "chunk_data = []\n",
    "seen_content = set()\n",
    "for i, c in enumerate(chunks):\n",
    "    content = c.page_content.replace(\"\\n\", \" \").strip()\n",
    "    if content not in seen_content:\n",
    "        seen_content.add(content)\n",
    "        chunk_data.append({\n",
    "            \"chunk_id\": i,\n",
    "            \"page\": c.metadata.get(\"page\") + 1,\n",
    "            \"text\": content\n",
    "        })\n",
    "df = pd.DataFrame(chunk_data)\n",
    "print(f\"Unique chunks: {len(df)}\")\n",
    "\n",
    "# Save to CSV\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "df.to_csv(\"data/processed/hr_policy_chunks.csv\", index=False)\n",
    "print(\"CSV written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded: Yes\n",
      "API Key preview: sk-pr...\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"API Key loaded:\", \"Yes\" if api_key else \"No\")\n",
    "if api_key:\n",
    "    print(\"API Key preview:\", api_key[:5] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event CollectionGetEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event CollectionGetEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 60 unique docs\n",
      "Docs in vector store: 60\n",
      "Sample metadata: {'chunk_id': 0, 'page': 1}\n",
      "Sample content: Policy Mandatory September  2012 The Nestlé   Human Resources Policy ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CHROMA_DISABLE_TELEMETRY\"] = \"1\"\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Load chunks\n",
    "df = pd.read_csv(\"data/processed/hr_policy_chunks.csv\")\n",
    "docs = [Document(page_content=row[\"text\"], metadata={\"page\": int(row[\"page\"]), \"chunk_id\": int(row[\"chunk_id\"])}) for _, row in df.iterrows()]\n",
    "print(f\"Loaded {len(docs)} docs\")\n",
    "\n",
    "# Create vector store\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vectordb = Chroma(collection_name=\"hr_policy_2012\", embedding_function=embeddings, persist_directory=\"db/chroma\")\n",
    "# Clear any existing data\n",
    "if vectordb.get()['ids']:\n",
    "    vectordb.delete_collection()\n",
    "    vectordb = Chroma(collection_name=\"hr_policy_2012\", embedding_function=embeddings, persist_directory=\"db/chroma\")\n",
    "# Add unique documents with unique IDs\n",
    "unique_docs = list({doc.page_content: doc for doc in docs}.values())\n",
    "ids = [f\"doc_{i}\" for i in range(len(unique_docs))]  # Unique IDs\n",
    "vectordb.add_documents(documents=unique_docs, ids=ids)\n",
    "print(f\"Stored {len(unique_docs)} unique docs\")\n",
    "\n",
    "# Verify content\n",
    "docs = vectordb.get()\n",
    "print(f\"Docs in vector store: {len(docs['ids'])}\")\n",
    "if docs['ids']:\n",
    "    print(\"Sample metadata:\", docs[\"metadatas\"][0])\n",
    "    print(\"Sample content:\", docs[\"documents\"][0][:100], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action\n",
    "\n",
    "\n",
    "\n",
    "1. **Imported Tools and Set Up Environment**  \n",
    "   - Configured Python, installed dependencies, and set up OpenAI API keys.\n",
    "\n",
    "2. **Loaded and Split Nestlé HR Policy PDF**  \n",
    "   - Used `PyPDFLoader` to load the 2012 HR policy document.\n",
    "   - Split the document into overlapping text chunks using `RecursiveCharacterTextSplitter` for better retrieval accuracy.\n",
    "\n",
    "3. **Created Vector Embeddings**  \n",
    "   - Converted chunks into numerical embeddings with OpenAI’s `text-embedding-ada-002`.\n",
    "   - Stored these in a ChromaDB vector store for semantic search.\n",
    "\n",
    "4. **Built a Question-Answering Chain**  \n",
    "   - Used `RetrievalQA` with `gpt-3.5-turbo` to generate context-aware answers.\n",
    "   - Designed a `PromptTemplate` requiring direct quotes and fallback responses when the document lacks relevant info.\n",
    "\n",
    "5. **Designed User Interface with Gradio**  \n",
    "   - Implemented a chatbot where users can type queries and receive answers with cited page references.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60 docs\n",
      "Stored 60 docs\n",
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query: what are total rewards?\n",
      "Returning history: 2 messages\n",
      "Processing query: what are working hours\n",
      "Returning history: 4 messages\n",
      "Processing query: how do I handle paid time off?\n",
      "Returning history: 6 messages\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CHROMA_DISABLE_TELEMETRY\"] = \"1\"\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import traceback\n",
    "\n",
    "# Load chunks\n",
    "df = pd.read_csv(\"data/processed/hr_policy_chunks.csv\")\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=row[\"text\"], \n",
    "        metadata={\"page\": int(row[\"page\"]), \"chunk_id\": int(row[\"chunk_id\"])}\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "print(f\"Loaded {len(docs)} docs\")\n",
    "\n",
    "# Create vector store\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vectordb = Chroma(\n",
    "    collection_name=\"hr_policy_2012\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"db/chroma\"\n",
    ")\n",
    "unique_docs = list({doc.page_content: doc for doc in docs}.values())\n",
    "ids = [f\"doc_{i}\" for i in range(len(unique_docs))]\n",
    "vectordb.add_documents(documents=unique_docs, ids=ids)\n",
    "print(f\"Stored {len(unique_docs)} docs\")\n",
    "\n",
    "# Prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"Answer using only the provided snippets. Quote exact text relevant to the question. \"\n",
    "        \"If the snippets do not directly address the question, say: \"\n",
    "        \"'I don’t know based on the 2012 HR Policy document.'\\n\\n\"\n",
    "        \"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Respond function for Gradio\n",
    "def respond(message, history=[]):\n",
    "    try:\n",
    "        print(f\"Processing query: {message}\")\n",
    "        if not message:\n",
    "            return history, \"Please enter a question.\"\n",
    "\n",
    "        result = qa_chain.invoke({\"query\": message})\n",
    "        answer = str(result[\"result\"])\n",
    "\n",
    "        if \"I don’t know\" not in answer.lower():\n",
    "            source_docs = result[\"source_documents\"]\n",
    "            seen = set()\n",
    "            unique_sources = [\n",
    "                doc for doc in source_docs\n",
    "                if not (doc.metadata[\"chunk_id\"] in seen or seen.add(doc.metadata[\"chunk_id\"]))\n",
    "            ]\n",
    "            if unique_sources:\n",
    "                pages = sorted({doc.metadata[\"page\"] for doc in unique_sources})\n",
    "                answer += f\"\\n\\n*Source: Page{'s' if len(pages) > 1 else ''} {', '.join(map(str, pages))}*\"\n",
    "        else:\n",
    "            answer = (\n",
    "                \"I don’t know based on the 2012 HR Policy document.\\n\\n\"\n",
    "                \"*Note: The 2012 policy lacks specific details on this topic. \"\n",
    "                \"Contact Nestlé HR for current policies.*\"\n",
    "            )\n",
    "\n",
    "        # Format for Gradio Chatbot with type=\"messages\"\n",
    "        history = history + [\n",
    "            {\"role\": \"user\", \"content\": str(message)},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ]\n",
    "\n",
    "        print(f\"Returning history: {len(history)} messages\")\n",
    "        return history, \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        traceback.print_exc()\n",
    "        history = history + [\n",
    "            {\"role\": \"user\", \"content\": str(message)},\n",
    "            {\"role\": \"assistant\", \"content\": error_msg}\n",
    "        ]\n",
    "        return history, \"\"\n",
    "\n",
    "# Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Nestlé HR Assistant\\nAsk about the 2012 HR Policy. Example: 'What are the total rewards?'\")\n",
    "    chatbot = gr.Chatbot(type=\"messages\")\n",
    "    txt = gr.Textbox(show_label=False, placeholder=\"Type question and hit enter\")\n",
    "    txt.submit(respond, [txt, chatbot], [chatbot, txt])\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradio version: 5.36.2\n"
     ]
    }
   ],
   "source": [
    "import gradio\n",
    "print(\"Gradio version:\", gradio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: total rewards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 5, Chunk 19: value and trust that our name brings to those  who work with us; the relationships with our line  ma...\n",
      "Page 5, Chunk 20: receive. Nestlé, therefore, focuses on Fixed Pay,  Variable Pay, Benefits, Personal Growth and  Deve...\n",
      "Page 5, Chunk 22: Nestlé Total Rewards programmes must be  established within the social and legal framework  of each ...\n",
      "Page 5, Chunk 24: transparency. Corporate policy:  Nestlé Total Rewards Policy We are committed to providing our emplo...\n",
      "Page 5, Chunk 18: The Nestlé Human Resources Policy 3  Total rewards Attracting new hires and keeping current  employe...\n",
      "\n",
      "Query: working hours\n",
      "Page 5, Chunk 30: employees is heard. Corporate policy:  Policy on Conditions of Work and Employment  Employment and w...\n",
      "Page 5, Chunk 28: and we insist that they also take steps so that  adequate working conditions are made available  to ...\n",
      "Page 7, Chunk 50: communication is established in the workplace.  While dialogue with trade unions is essential, it  d...\n",
      "Page 5, Chunk 23: within the framework of Company policy.  Sufficient time should be spent with each  employee to expl...\n",
      "Page 1, Chunk 0: Policy Mandatory September  2012 The Nestlé   Human Resources Policy...\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vectordb = Chroma(collection_name=\"hr_policy_2012\", embedding_function=embeddings, persist_directory=\"db/chroma\")\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "for query in [\"total rewards\", \"working hours\"]:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    docs = retriever.invoke(query)\n",
    "    for d in docs:\n",
    "        print(f\"Page {d.metadata['page']}, Chunk {d.metadata['chunk_id']}: {d.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: total rewards\n",
      "Page 5, Chunk 19: value and trust that our name brings to those  who work with us; the relationships with our line  ma...\n",
      "Page 5, Chunk 20: receive. Nestlé, therefore, focuses on Fixed Pay,  Variable Pay, Benefits, Personal Growth and  Deve...\n",
      "Page 5, Chunk 22: Nestlé Total Rewards programmes must be  established within the social and legal framework  of each ...\n",
      "Page 5, Chunk 24: transparency. Corporate policy:  Nestlé Total Rewards Policy We are committed to providing our emplo...\n",
      "Page 5, Chunk 18: The Nestlé Human Resources Policy 3  Total rewards Attracting new hires and keeping current  employe...\n",
      "\n",
      "Query: working hours\n",
      "Page 5, Chunk 30: employees is heard. Corporate policy:  Policy on Conditions of Work and Employment  Employment and w...\n",
      "Page 5, Chunk 28: and we insist that they also take steps so that  adequate working conditions are made available  to ...\n",
      "Page 7, Chunk 50: communication is established in the workplace.  While dialogue with trade unions is essential, it  d...\n",
      "Page 5, Chunk 23: within the framework of Company policy.  Sufficient time should be spent with each  employee to expl...\n",
      "Page 1, Chunk 0: Policy Mandatory September  2012 The Nestlé   Human Resources Policy...\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vectordb = Chroma(collection_name=\"hr_policy_2012\", embedding_function=embeddings, persist_directory=\"db/chroma\")\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "for query in [\"total rewards\", \"working hours\"]:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    docs = retriever.invoke(query)\n",
    "    for d in docs:\n",
    "        print(f\"Page {d.metadata['page']}, Chunk {d.metadata['chunk_id']}: {d.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response: Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    response = llm.invoke(\"Test query: Say 'Hello'\")\n",
    "    print(\"API Response:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"OpenAI API error: {str(e)}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration: Sample Query\n",
    "\n",
    "Below is an example query run through the question-answering chain, showing how the chatbot retrieves an answer and cites the HR policy document. This demonstrates the system’s functionality even outside the Gradio interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Question: What are the total rewards mentioned in the HR policy?\n",
      "\n",
      "Chatbot Answer:\n",
      "Nestlé focuses on Fixed Pay, Variable Pay, Benefits, Personal Growth and Development and Work Life Environment as the key elements that define Total Rewards.\n",
      "\n",
      "Source Pages: 5\n"
     ]
    }
   ],
   "source": [
    "# Example query for demonstration\n",
    "try:\n",
    "    sample_question = \"What are the total rewards mentioned in the HR policy?\"\n",
    "    print(f\"User Question: {sample_question}\\n\")\n",
    "\n",
    "    result = qa_chain.invoke({\"query\": sample_question})\n",
    "    print(\"Chatbot Answer:\")\n",
    "    print(result[\"result\"])\n",
    "\n",
    "    # Show source documents/pages\n",
    "    source_docs = result.get(\"source_documents\", [])\n",
    "    if source_docs:\n",
    "        pages = sorted({doc.metadata[\"page\"] for doc in source_docs})\n",
    "        print(f\"\\nSource Pages: {', '.join(map(str, pages))}\")\n",
    "    else:\n",
    "        print(\"\\nNo source documents returned.\")\n",
    "except Exception as e:\n",
    "    print(\"Error during sample query:\", str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration: Fallback Query\n",
    "\n",
    " - The chatbot is designed to admit when the HR policy does not contain the requested information.  \n",
    " - This prevents misinformation and directs users to contact HR for topics not covered in the 2012 policy.  \n",
    " - Below is an example of such a query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Question: Does the HR policy mention remote work guidelines?\n",
      "\n",
      "Chatbot Answer:\n",
      "I don’t know based on the 2012 HR Policy document.\n",
      "\n",
      "Source Pages: 1, 5\n"
     ]
    }
   ],
   "source": [
    "# Example query that likely won't be in the 2012 HR Policy\n",
    "try:\n",
    "    fallback_question = \"Does the HR policy mention remote work guidelines?\"\n",
    "    print(f\"User Question: {fallback_question}\\n\")\n",
    "\n",
    "    result = qa_chain.invoke({\"query\": fallback_question})\n",
    "    print(\"Chatbot Answer:\")\n",
    "    print(result[\"result\"])\n",
    "\n",
    "    # Show source docs if any (there might be none)\n",
    "    source_docs = result.get(\"source_documents\", [])\n",
    "    if source_docs:\n",
    "        pages = sorted({doc.metadata[\"page\"] for doc in source_docs})\n",
    "        print(f\"\\nSource Pages: {', '.join(map(str, pages))}\")\n",
    "    else:\n",
    "        print(\"\\nNo relevant source documents returned (as expected).\")\n",
    "except Exception as e:\n",
    "    print(\"Error during fallback query:\", str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "\n",
    "The completed project delivers a functional **AI-powered HR Assistant** that answers questions from Nestlé’s 2012 HR Policy.  \n",
    "- **User Experience:** Employees can ask free-form questions in a Gradio chatbot and receive clear, concise answers.  \n",
    "- **Accuracy:** Responses are generated using semantic retrieval from the policy document, ensuring relevant and sourced information.  \n",
    "- **Transparency:** The chatbot cites the exact policy page(s) for each answer, helping employees trust the results.  \n",
    "- **Fallback Handling:** If the document does not contain the answer, the chatbot informs the user and suggests contacting HR for current policies.  \n",
    "\n",
    "This assistant demonstrates how conversational AI can improve HR efficiency by reducing manual search time and providing a reliable first point of contact for policy-related queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Demonstration Summary\n",
    "\n",
    "## **Nestlé HR Assistant** according to the project requirements.\n",
    "\n",
    "### Demonstration Outcomes\n",
    "- **Sample Query (Success)**  \n",
    "  - Question: *What are the total rewards mentioned in the HR policy?*  \n",
    "  - The assistant retrieved the correct answer from the 2012 policy and cited the relevant page(s).  \n",
    "- **Fallback Query (Graceful Handling)**  \n",
    "  - Question: *Does the HR policy mention remote work guidelines?*  \n",
    "  - The assistant responded responsibly with *I don’t know based on the 2012 HR Policy document*, reminding users to contact HR for more current information.\n",
    "\n",
    "### Key Features Implemented\n",
    "- Loaded and processed Nestlé’s 2012 HR Policy PDF.  \n",
    "- Split the text into manageable chunks for semantic search.  \n",
    "- Generated embeddings with `text-embedding-ada-002` and stored them in **ChromaDB**.  \n",
    "- Built a **RetrievalQA** chain using `gpt-3.5-turbo`.  \n",
    "- Created a **Gradio chatbot interface** for interactive use.  \n",
    "- Added logic for **accurate citations** and **fallback responses**.\n",
    "\n",
    "### Conclusion\n",
    "The HR Assistant demonstrates how conversational AI can enhance HR operations by:\n",
    "- Reducing time spent searching policy documents.  \n",
    "- Providing reliable, cited answers.  \n",
    "- Handling out-of-scope questions responsibly.  \n",
    "\n",
    "This fulfills the **Situation, Task, Action, and Result** framework and meets the project’s goals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
